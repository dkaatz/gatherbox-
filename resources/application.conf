akka.kafka.consumer {

  # Tuning property of scheduled polls.
  //poll-interval = 50ms

  # Tuning property of the `KafkaConsumer.poll` parameter.
  # Note that non-zero value means that blocking of the thread that
  # is executing the stage will be blocked.
  //poll-timeout = 50ms

  // This configures how long the consumer will keep tring to connect. Instead of making this number very large we use
  // a smaller number so that an error message is emitted indicating there is a problem with the connection. The
  // consumer is restarted automatically after stopping and emitting the error.
  max-wakeups = 5

  // when deployed in aws, with the default 3s wakeup-timeout there are periods of time where we get repeated WakeupExceptions
  // which eventually trigger a failure and restart of the stream. This causes scenarios where a message is delivered
  // more than once since we aren't using an at-most-once consumer.
  wakeup-timeout = 6s

  kafka-clients {
    auto.offset.reset: "earliest"
    max.poll.interval.ms: 20000 // default 30000
    fetch.min.bytes: 0
    fetch.max.wait.ms: 10000

    // Trying to avoid this error by reducing max poll records from the default...
    // org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already
    // rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to
    // poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is
    // spending too much time message processing. You can address this either by increasing the session timeout or by
    // reducing the maximum size of batches returned in poll() with max.poll.records.
    max.poll.records: 100 // default = 500 (old 2147483647)
  }
}